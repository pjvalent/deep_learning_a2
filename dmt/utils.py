"""
Do not edit this file, it contains utilities from the Transformer lecture and will be replaced by the instructor's copy
Feel free to use these utilities in your own submission.py, but it is not required
"""
import torch as tr

# Embeds a sequence of tokens as one-hot vectors
# offset=1 is used to shift an output sequence by one position
def embed(seq, max_len, embeddings, lookup, offset=0):
    embedded = tr.zeros(max_len, embeddings.shape[1])
    cap = min(len(seq), max_len-offset)
    if cap > 0:
        embedded[offset:offset+cap] = tr.stack(tuple(embeddings[lookup[token]] for token in seq[:cap]))
    return embedded

# Encoder function factory
# Encodes positions using one-hot vectors, concatenated with token embeddings
def one_hot_positional_encoder(max_len):
    I = tr.eye(max_len)
    def encode_position(inputs):
        return tr.cat((I, inputs), dim=1)
    return encode_position

# Core self-attention mechanism
# masked=True prevents query at time t from attending to keys predicted later than time t
def Attention(Q, K, V, masked=False):
    dk = Q.shape[1]
    logits = Q @ K.t() / dk**.5
    if masked:
        idx = tr.arange(Q.shape[0])
        logits[idx.unsqueeze(1) < idx] = -tr.inf
    return tr.softmax(logits, dim=1) @ V

# Multi-head attention Module
# Wraps multiple attention heads with learnable projections, skip connections, and layer normalization
# projections can be a string containing any of "Q", "K", "V", "O"
# for query, key, value, and output projections, respectively
class MultiHeadAttention(tr.nn.Module):
    def __init__(self, num_heads, d_model, masked=False, projections=""):
        super(MultiHeadAttention, self).__init__()
        dh = d_model // num_heads
        self.masked = masked
        self.num_heads = num_heads
        self.WQ, self.WK, self.WV = tuple(
            tr.nn.ModuleList([tr.nn.Linear(d_model, dh, bias=False) for i in range(num_heads)])
            if p in projections else [lambda x: x[:,:dh]]*num_heads
            for p in "QKV")
        self.WO = tr.nn.Linear(dh * num_heads, d_model, bias=False) if "O" in projections else lambda x: x
        self.ln = tr.nn.LayerNorm(d_model)
        self.projections = projections

    def forward(self, Q, K, V):
        heads = [
            Attention(self.WQ[i](Q), self.WK[i](K), self.WV[i](V), self.masked)
            for i in range(self.num_heads)]
        out = self.WO(tr.cat(heads, dim=1))
        out += Q # skip connection
        out = self.ln(out) # layer normalization
        return out
